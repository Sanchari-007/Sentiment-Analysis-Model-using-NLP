# -*- coding: utf-8 -*-
"""NLP_Sentiment_Analysis_Positive_Neutral_Negative(renewed).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13R2SELJAnfUQ62Ijc66WImQYNOwl9u1A
"""

!pip install emoji

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
nltk.download('stopwords')
from nltk.corpus import stopwords
STOPWORDS = set(stopwords.words('english'))
from string import punctuation
import emoji
import spacy
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score
from wordcloud import WordCloud
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
import pickle
import re

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

STOPWORDS = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

'''from google.colab import drive
drive.mount('/content/drive')'''

"""**Exploratory Data Analysis**"""

# Load the data
data = pd.read_csv("/content/AI-Hackathon-test-data-set.csv", encoding='latin1')
data.head()

# Split the "Comments,Sentiment" column
split_data = data['"Comments,Sentiment"'].str.strip('"').str.rsplit(',', n=1, expand=True)
data['Comments'] = split_data[0]
data['Sentiment'] = split_data[1]
data.drop('"Comments,Sentiment"', axis=1, inplace=True)

# 1. Handling Missing Values:
data.dropna(inplace=True) # Drop rows with missing values

# Dictionary for slang and abbreviation expansion (add more as needed)
slang_dict = {"lol": "laughing out loud", "btw": "by the way", "u": "you", "r": "are"}
def clean_text(text):
    # Lowercase the text
    text = text.lower()
    # Remove URLs
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)
    # Remove Emojis
    text = emoji.replace_emoji(text, replace="")
    # Expand Slang and Abbreviations
    for slang, expansion in slang_dict.items():
        text = text.replace(slang, expansion)
    # Remove non-alphanumeric characters and keep spaces
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Remove extra spaces
    text = re.sub(r'\s+', ' ', text).strip()
    # Tokenize the text
    words = text.split()
    # Remove stop words and lemmatize
    words = [lemmatizer.lemmatize(word) for word in words if word not in STOPWORDS]
    # Join the words back into a string
    text = ' '.join(words)
    return text

# Load spaCy model (you might need to download it first: `python -m spacy download en_core_web_sm`)
nlp = spacy.load("en_core_web_sm")

def handle_negation_with_dependency(text):
    doc = nlp(text)
    negated_tokens = []
    for token in doc:
        if token.dep_ == "neg":  # Check if token is a negation cue
            for child in token.children:
                negated_tokens.append(child.text)

    # Modify the text based on negated tokens (example: add "NOT_" prefix)
    modified_text = []
    for token in doc:
        if token.text in negated_tokens:
            modified_text.append("NOT_" + token.text)
        else:
            modified_text.append(token.text)

    return " ".join(modified_text)

data['Comments'] = data['Comments'].apply(handle_negation_with_dependency)

data['Comments'] = data['Comments'].apply(clean_text) # Apply the clean_text function
data['Comments'] = data['Comments'].apply(handle_negation_with_dependency) # Apply negation handling
data
data['Sentiment'].value_counts()

'''# 3. Stemming (Optional but can improve accuracy):
stemmer = PorterStemmer()
data['Comments'] = data['Comments'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))'''

data.to_csv('cleaned_data.csv', index=False)

# 4. Feature Extraction:
# Using TF-IDF Vectorizer (often performs better than CountVectorizer)
'''vectorizer = TfidfVectorizer(max_features=2500) # Adjust max_features
X = vectorizer.fit_transform(data['Comments']).toarray()
y = data['Sentiment']'''
# 1. Incorporate N-grams in TF-IDF
vectorizer = TfidfVectorizer(max_features=2500, ngram_range=(1, 2))  # Include unigrams and bigrams
X = vectorizer.fit_transform(data['Comments']).toarray()

import os
os.makedirs('Models', exist_ok=True)
pickle.dump(vectorizer, open('Models/TfidfVectorizer.pkl', 'wb'))

# 5. Data Splitting:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=15)

# Assuming X_test is a NumPy array or a similar data structure
X_test_df = pd.DataFrame(X_test)  # Convert to DataFrame if needed

# Save to a CSV file
X_test_df.to_csv('X_test.csv', index=False)

# 6. Scaling (Important for Random Forest):
scaler = MinMaxScaler()
X_train_scl = scaler.fit_transform(X_train)
X_test_scl = scaler.transform(X_test)
#Saving the scaler model
pickle.dump(scaler, open('Models/scaler.pkl', 'wb'))

# 7. Random Forest Model:
model_rf = RandomForestClassifier(random_state=15)  # Add random_state for reproducibility
model_rf.fit(X_train_scl, y_train)

# 8. Evaluation:
y_pred = model_rf.predict(X_test_scl)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

# 9. Confusion Matrix:
cm = confusion_matrix(y_test, y_pred)
cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_rf.classes_)
cm_display.plot()
plt.show()

# 10. Hyperparameter Tuning using GridSearchCV:
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(estimator=model_rf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train_scl, y_train)

# Evaluate the model with the best parameters
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test_scl)
print("Accuracy (Best Model):", accuracy_score(y_test, y_pred_best))
print(classification_report(y_test, y_pred_best))

# Save the best model to a file
model_rf = 'best_random_forest_model.pkl'
pickle.dump(best_model, open(model_rf, 'wb'))

# Load the saved model
model_rf = 'best_random_forest_model.pkl'
loaded_model = pickle.load(open(model_rf, 'rb'))

# Preprocess new data (using the same steps as before)
new_data = pd.DataFrame({'Comments': ['Good day it is']})
new_data['Comments'] = new_data['Comments'].apply(clean_text) # Assuming clean_text function is defined
new_data['Comments'] = new_data['Comments'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))
X_new = vectorizer.transform(new_data['Comments']).toarray()
X_new_scl = scaler.transform(X_new)

# Make predictions
predictions = loaded_model.predict(X_new_scl)
print(predictions)